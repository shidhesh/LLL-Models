{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jNZy8wMfCQ8M"
   },
   "source": [
    "## Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "17a7aYKQCT5O"
   },
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "tLvsgdAqEsqv"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline, set_seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W3ZQED8WFv1v"
   },
   "source": [
    "gpt2<br>\n",
    "gpt2-medium<br>\n",
    "gpt2-large<br>\n",
    "gpt2-xl<br>\n",
    "distilgpt2<br>\n",
    "microsoft/DialoGPT-small<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wOHKeMjuCXva"
   },
   "source": [
    "## Load GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LwhSoCcsEsto",
    "outputId": "b432922a-5a0b-48ff-b88c-714c34eb2309"
   },
   "outputs": [],
   "source": [
    "generator = pipeline('text-generation', model='gpt2')\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ghW00rKvCdRO"
   },
   "source": [
    "## Generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j4l8sfkQEsxh",
    "outputId": "cfbf66e7-0f9c-4ec9-8feb-8d4824746e63"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"Hello, I'm a language model, but what I'm really doing is making a human-readable document. There are other languages, but those are\"},\n",
       " {'generated_text': \"Hello, I'm a language model, not a syntax model. That's why I like it. I've done a lot of programming projects.\\n\"},\n",
       " {'generated_text': \"Hello, I'm a language model, and I'll do it in no time!\\n\\nOne of the things we learned from talking to my friend\"},\n",
       " {'generated_text': \"Hello, I'm a language model, not a command line tool.\\n\\nIf my code is simple enough:\\n\\nif (use (string\"},\n",
       " {'generated_text': \"Hello, I'm a language model, I've been using Language in all my work. Just a small example, let's see a simplified example.\"}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=5)                                                                                      #<|startoftext|>  <|endoftext|>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J6JdlGnQEs0K",
    "outputId": "06fff383-e3ce-4a69-f975-de4db4fe85f4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': '1,2,3,4,5,'},\n",
       " {'generated_text': '1,2,3,4,6,'},\n",
       " {'generated_text': '1,2,3,4,5,'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(\"1,2,3,4,\", max_length=10, num_return_sequences=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vxJUPTwIEs28",
    "outputId": "0a582b55-e2ac-47ba-9354-0de1c8a0d7ef"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'The countries of the Asia Union are:\\n1. Indonesia\\n2. Japan\\n3. Myanmar\\n4. Cambodia\\n5. Thailand\\n6. Thailand\\n7.'},\n",
       " {'generated_text': 'The countries of the Asia Union are:\\n1. Indonesia\\n2. Japan\\n3. Myanmar\\n4. Malaysia\\n5. Sri Lanka\\n6. South Korea\\n'},\n",
       " {'generated_text': 'The countries of the Asia Union are:\\n1. Indonesia\\n2. Japan\\n3. Myanmar\\n4. Taiwan\\n5. Indonesia\\n6. Philippines\\n7.'},\n",
       " {'generated_text': 'The countries of the Asia Union are:\\n1. Indonesia\\n2. Japan\\n3. Myanmar\\n4. Turkmenistan.\\n5. India\\n6. Pakistan'},\n",
       " {'generated_text': 'The countries of the Asia Union are:\\n1. Indonesia\\n2. Japan\\n3. Myanmar\\n4. Vietnam\\n5. Singapore\\n6. Taiwan\\n7.'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(\"The countries of the Asia Union are:\\n1. Indonesia\\n2. Japan\\n3. Myanmar\\n4.\", max_length=35, num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mhDBy1wxEs7F",
    "outputId": "d53eee62-e8db-481d-d35e-6109409940b4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'The capital of Japan is Tokyo, The capital of Egypt is Egypt.'},\n",
       " {'generated_text': 'The capital of Japan is Tokyo, The capital of Egypt is Cairo'},\n",
       " {'generated_text': 'The capital of Japan is Tokyo, The capital of Egypt is Cairo'},\n",
       " {'generated_text': 'The capital of Japan is Tokyo, The capital of Egypt is Cairo'},\n",
       " {'generated_text': 'The capital of Japan is Tokyo, The capital of Egypt is Alexandria'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(\"The capital of Japan is Tokyo, The capital of Egypt is\", max_length=13, num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lEJopDcJkXXJ",
    "outputId": "8a81a3a3-dd8c-4087-af24-2330934e91b4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': '1. these are features=this is a feature\\n 2.these are players = this is a feature\\n\\n3.'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(\"1. these are features=this is a feature\\n 2.these are players = this is a\", max_length=25, num_return_sequences=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VeB2_k-7gLBH",
    "outputId": "51fb6966-1585-4dfc-83c3-f3e6ad65167f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'A:Hello\\n B: Hi, my name sidheshwar\\nA: hey im GPT-2? B: yes B: where are you\\nA: ikk B:'},\n",
       " {'generated_text': \"A:Hello\\n B: Hi, my name sidheshwar\\nA: hey im GPT-2? B: Yeah that's right\\nA: Hey my name is Sidhesh\"},\n",
       " {'generated_text': 'A:Hello\\n B: Hi, my name sidheshwar\\nA: hey im GPT-2? B: Sure B: Hey, how are you doing GPT-2.'},\n",
       " {'generated_text': 'A:Hello\\n B: Hi, my name sidheshwar\\nA: hey im GPT-2? B: Yes\\nA: and i do not care about you\\nB:'},\n",
       " {'generated_text': 'A:Hello\\n B: Hi, my name sidheshwar\\nA: hey im GPT-2? B: Yes?\\nB: and who knows, is there a real website'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(\"A:Hello\\n B: Hi, my name sidheshwar\\nA: hey im GPT-2? B:\", max_length=40, num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zPXwSRPjHXje"
   },
   "source": [
    "## Get Features from GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1KhGXTdyHS-2"
   },
   "outputs": [],
   "source": [
    "from transformers import TFGPT2Model\n",
    "model = TFGPT2Model.from_pretrained('distilgpt2')\n",
    "\n",
    "from transformers import GPT2Tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')\n",
    "#model = GPT2Model.from_pretrained('distilgpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EFD_XWvCHTDi"
   },
   "outputs": [],
   "source": [
    "text = \"This is a test text.\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eC3qouGXHTK8",
    "outputId": "12d69c5d-7731-45ac-d67c-f608a56f9f63"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 768])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1O-FS5eZ2trs",
    "outputId": "6ca92908-cdeb-4cea-d60b-b5f2bfbc5d6d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[1212,  318,  257, 1332, 2420,   13]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_e2euPOhGjjk"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
