{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cRGFsVRICNgL"
   },
   "source": [
    "# Chat with your Documents\n",
    "\n",
    "We will chat with this nice article titled Transformers without pain ðŸ¤— using:\n",
    "\n",
    "- Palm Model\n",
    "- LangChian\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jNZy8wMfCQ8M"
   },
   "source": [
    "## Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-U6egfDiEdfP"
   },
   "outputs": [],
   "source": [
    "!pip -q install configparser langchain google-generativeai chromadb\n",
    "!pip -q install transformers huggingface_hub sentence_transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "17a7aYKQCT5O"
   },
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tLvsgdAqEsqv"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain.llms import GooglePalm\n",
    "from langchain.embeddings import GooglePalmEmbeddings\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.indexes import VectorstoreIndexCreator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wOHKeMjuCXva"
   },
   "source": [
    "## Load Palm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f0lwktKeUeLp"
   },
   "outputs": [],
   "source": [
    "load_dotenv(find_dotenv()) # GOOGLE_API_KEY = \"do not share your key\"\n",
    "api_key = os.environ[\"GOOGLE_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LwhSoCcsEsto"
   },
   "outputs": [],
   "source": [
    "Palm_llm = GooglePalm(temperature=0.1, max_output_tokens=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 892,
     "status": "ok",
     "timestamp": 1688532331832,
     "user": {
      "displayName": "Ibrahim Sobh",
      "userId": "02864663038268916670"
     },
     "user_tz": -180
    },
    "id": "t2Lx_ejpPWWD",
    "outputId": "d7d285db-9d7f-4c92-9007-5a2fbdef9bba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Justin Bieber was born on March 1, 1994. The New England Patriots won Super Bowl XXXVI in 2002.\n",
      "The final answer: New England Patriots.\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "template = \"\"\"Question: {question}\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt_open = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "open_chain = LLMChain(prompt=prompt_open,llm = Palm_llm)\n",
    "\n",
    "question = \"What NFL team won the Super Bowl in the year Justin Beiber was born?\"\n",
    "print(open_chain.run(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ghW00rKvCdRO"
   },
   "source": [
    "## Load and index your Doc(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8gx_FIDfSVP9"
   },
   "outputs": [],
   "source": [
    "# load docs and construct the index\n",
    "urls = ['https://www.linkedin.com/pulse/transformers-without-pain-ibrahim-sobh-phd/',]\n",
    "loader = WebBaseLoader(urls)\n",
    "index = VectorstoreIndexCreator(\n",
    "        embedding=GooglePalmEmbeddings(),\n",
    "        text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0, separators=[\" \", \",\", \"\\n\"])).from_loaders([loader])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "evHDF8bhTKg7"
   },
   "outputs": [],
   "source": [
    "# QA Retrieval\n",
    "qa_retriever = RetrievalQA.from_chain_type(llm=Palm_llm, chain_type=\"stuff\",\n",
    "                                    retriever=index.vectorstore.as_retriever(),\n",
    "                                    input_key=\"question\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VEYAsqqnQe0z"
   },
   "source": [
    "## Ask your documents questions and get answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1042,
     "status": "ok",
     "timestamp": 1688535000282,
     "user": {
      "displayName": "Ibrahim Sobh",
      "userId": "02864663038268916670"
     },
     "user_tz": -180
    },
    "id": "8WIHO3xqQFFv",
    "outputId": "ff10f687-d37c-404d-be37-3ad7b864bd4e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What these documents are about?',\n",
       " 'result': 'The documents are about transformers, which are a type of neural network that has been used successfully in natural language processing and computer vision tasks.'}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_retriever(\"What these documents are about?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1545,
     "status": "ok",
     "timestamp": 1688535031767,
     "user": {
      "displayName": "Ibrahim Sobh",
      "userId": "02864663038268916670"
     },
     "user_tz": -180
    },
    "id": "PEoS1CYaQFLN",
    "outputId": "809dae58-5895-48ef-94b4-222d8a1536f8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What is the main idea of transformers?',\n",
       " 'result': 'The main idea of transformers is to use attention mechanisms to model long-range dependencies in sequences.'}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_retriever(\"What is the main idea of transformers?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2124,
     "status": "ok",
     "timestamp": 1688535086419,
     "user": {
      "displayName": "Ibrahim Sobh",
      "userId": "02864663038268916670"
     },
     "user_tz": -180
    },
    "id": "pxqw153kPt0u",
    "outputId": "5042d8be-4222-47c5-82d1-edc71c013d3d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Why transformers are better thanRNNs and CNNs?',\n",
       " 'result': 'Transformers are better than RNNs and CNNs because they are more parallelizable and require significantly less time to train.'}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_retriever(\"Why transformers are better thanRNNs and CNNs?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1816,
     "status": "ok",
     "timestamp": 1688535169317,
     "user": {
      "displayName": "Ibrahim Sobh",
      "userId": "02864663038268916670"
     },
     "user_tz": -180
    },
    "id": "SS1GxU4kPt4r",
    "outputId": "9f352630-4b05-457b-8e13-be106944d7ab"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'what is positional encoding?',\n",
       " 'result': 'Positional encoding is a technique used to represent the order of words in a sequence.'}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_retriever(\"what is positional encoding?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2091,
     "status": "ok",
     "timestamp": 1688535733034,
     "user": {
      "displayName": "Ibrahim Sobh",
      "userId": "02864663038268916670"
     },
     "user_tz": -180
    },
    "id": "HTYcRxz0kfKT",
    "outputId": "1e95d1d7-7621-4576-a090-7a2f89e6fdc4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'How to represent the order of words?',\n",
       " 'result': 'Positional Encoding is used to represent the order of the sequence.'}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_retriever(\"How to represent the order of words?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1305,
     "status": "ok",
     "timestamp": 1688535191634,
     "user": {
      "displayName": "Ibrahim Sobh",
      "userId": "02864663038268916670"
     },
     "user_tz": -180
    },
    "id": "FHz33pnwibXR",
    "outputId": "273a237a-e343-450f-f1ac-828a2f9e9274"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'what is self attention?',\n",
       " 'result': 'Self attention is a technique to compute a weighted sum of the values (in the encoder), dependent on another value (in the decoder).'}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_retriever(\"what is self attention?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2165,
     "status": "ok",
     "timestamp": 1688535777514,
     "user": {
      "displayName": "Ibrahim Sobh",
      "userId": "02864663038268916670"
     },
     "user_tz": -180
    },
    "id": "k6l_TeQDkuEa",
    "outputId": "19fd356a-7ee5-44ff-8ad3-17869e5a8805"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'How attention is used in encoder and decoder?',\n",
       " 'result': 'Attention is used in both encoder and decoder. In the encoder, attention is used to compute a weighted sum of the values (in the encoder), dependent on another value (in the decoder). In the decoder, attention is used to attend or focus on values from the encoder.'}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_retriever(\"How attention is used in encoder and decoder?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1817,
     "status": "ok",
     "timestamp": 1688535234104,
     "user": {
      "displayName": "Ibrahim Sobh",
      "userId": "02864663038268916670"
     },
     "user_tz": -180
    },
    "id": "S9gbtLQ6ibZz",
    "outputId": "4d7f2684-908b-4624-b737-2d8ce7915100"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'what is attention used between encoder and decoder and why?',\n",
       " 'result': 'Encoder-decoder attention is used to allow the decoder to attend to values from the encoder. This is done so that the decoder can learn the relationship between the input and output sequences.'}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_retriever(\"what is attention used between encoder and decoder and why?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2195,
     "status": "ok",
     "timestamp": 1688535388327,
     "user": {
      "displayName": "Ibrahim Sobh",
      "userId": "02864663038268916670"
     },
     "user_tz": -180
    },
    "id": "9egm_PSgjMNc",
    "outputId": "ebf245e4-08b0-4c1f-a3c4-351916ed52cf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'How query, key, and value vectors are used?',\n",
       " 'result': 'The query vector is used to compute a weighted sum of the values through the keys. Specifically: q dot product all the keys, then softmax to get weights and finally use these weights to compute a weighted sum of the values.'}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_retriever(\"How query, key, and value vectors are used?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1335,
     "status": "ok",
     "timestamp": 1688535299375,
     "user": {
      "displayName": "Ibrahim Sobh",
      "userId": "02864663038268916670"
     },
     "user_tz": -180
    },
    "id": "Q55it-Aqibdv",
    "outputId": "16c73ba1-2be1-4701-d343-0a46ec0f11f8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'How to start using transformers?',\n",
       " 'result': 'To start using transformers, you can use the huggingface/transformers library. This library provides thousands of pretrained models to perform tasks on texts such as classification, information extraction, question answering, summarization, translation, text generation, etc in 100+ languages.'}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_retriever(\"How to start using transformers?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ndf-xQS6ibjY"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
